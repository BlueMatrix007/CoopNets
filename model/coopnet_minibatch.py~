from __future__ import division

import os
import math

import numpy as np
from progressbar import ETA, Bar, Percentage, ProgressBar

import optim
from utils.custom_ops import *
from utils.data_io import DataSet, saveSampleResults


class CoopNet(object):
    def __init__(self, config):
        self.type = 'object'
        self.batch_size = config.batch_size
        self.image_size = config.image_size
        self.num_chain = config.num_chain
        self.sample_steps = config.sample_steps

        self.d_lr = config.d_lr
        self.g_lr = config.g_lr
        self.beta1 = 0.5
        self.delta = config.delta
        self.refsig = 0.016
        self.sigma = config.sigma

        self.num_epochs = config.num_epochs
        self.data_path = os.path.join(config.data_path, config.category)
        self.log_step = config.log_step
        self.output_dir = os.path.join(config.output_dir, config.category)

        self.log_dir = os.path.join(self.output_dir, 'log')
        self.sample_dir = os.path.join(self.output_dir, 'synthesis')
        self.interp_dir = os.path.join(self.output_dir, 'interpolation')
        self.model_dir = os.path.join(self.output_dir, 'checkpoints')

        if tf.gfile.Exists(self.log_dir):
            tf.gfile.DeleteRecursively(self.log_dir)
        tf.gfile.MakeDirs(self.log_dir)

        if self.type == 'texture':
            self.z_size = 49
        elif self.type == 'object':
            self.z_size = 100
        elif self.type == 'object_small':
            self.z_size = 2

        self.syn = tf.placeholder(shape=[None, self.image_size, self.image_size, 3], dtype=tf.float32)
        self.obs = tf.placeholder(shape=[None, self.image_size, self.image_size, 3], dtype=tf.float32)
        self.z = tf.placeholder(shape=[None, self.z_size], dtype=tf.float32)

    def descriptor(self, inputs, reuse=False):
        with tf.variable_scope('des', reuse=reuse):
            if self.type == 'texture':
                conv1 = conv2d(inputs, 100, kernal=(15, 15), strides=(3, 3), padding="SAME", name="conv1")
                conv1 = tf.nn.relu(conv1)

                conv2 = conv2d(conv1, 70, kernal=(9, 9), strides=(1, 1), padding="SAME", name="conv2")
                conv2 = tf.nn.relu(conv2)

                conv3 = conv2d(conv2, 30, kernal=(7, 7), strides=(1, 1), padding="SAME", name="conv3")
                conv3 = tf.nn.relu(conv3)

                return conv3
            elif self.type == 'object':
                conv1 = conv2d(inputs, 64, kernal=(5, 5), strides=(2, 2), padding="SAME", activate_fn=leaky_relu,
                               name="conv1")

                conv2 = conv2d(conv1, 128, kernal=(3, 3), strides=(2, 2), padding="SAME", activate_fn=leaky_relu,
                               name="conv2")

                conv3 = conv2d(conv2, 256, kernal=(3, 3), strides=(1, 1), padding="SAME", activate_fn=leaky_relu,
                               name="conv3")

                fc = fully_connected(conv3, 100, name="fc")

                return fc
            elif self.type == 'object_small':
                conv1 = conv2d(inputs, 64, kernal=(4, 4), strides=(2, 2), padding="SAME", name="conv1")
                conv1 = tf.nn.relu(conv1)

                conv2 = conv2d(conv1, 128, kernal=(2, 2), strides=(1, 1), padding="SAME", name="conv2")
                conv2 = tf.nn.relu(conv2)

                fc = fully_connected(conv2, 1, name="fc")

                return fc
            else:
                return NotImplementedError

    def generator(self, inputs, reuse=False, is_training=True):
        with tf.variable_scope('gen', reuse=reuse):
            if self.type == 'texture':
                inputs = tf.reshape(inputs, (-1, 7, 7, 1))
            	convt1 = convt2d(inputs, (None, self.image_size // 16, self.image_size // 16, 512), kernal=(5, 5)
                             , strides=(2, 2), padding="SAME", name="convt1")

            elif self.type == 'object' or self.type == 'object_small':
                inputs = tf.reshape(inputs, [-1, 1, 1, self.z_size])
            	convt1 = convt2d(inputs, (None, self.image_size // 16, self.image_size // 16, 512), kernal=(4, 4)
                             , strides=(1, 1), padding="VALID", name="convt1")
            else:
                return NotImplementedError

            convt1 = tf.contrib.layers.batch_norm(convt1, is_training=is_training)
            convt1 = leaky_relu(convt1)

            convt2 = convt2d(convt1, (None, self.image_size // 8, self.image_size // 8, 256), kernal=(5, 5)
                             , strides=(2, 2), padding="SAME", name="convt2")
            convt2 = tf.contrib.layers.batch_norm(convt2, is_training=is_training)
            convt2 = leaky_relu(convt2)

            convt3 = convt2d(convt2, (None, self.image_size // 4, self.image_size // 4, 128), kernal=(5, 5)
                             , strides=(2, 2), padding="SAME", name="convt3")
            convt3 = tf.contrib.layers.batch_norm(convt3, is_training=is_training)
            convt3 = leaky_relu(convt3)

            convt4 = convt2d(convt3, (None, self.image_size // 2, self.image_size // 2, 64), kernal=(5, 5)
                             , strides=(2, 2), padding="SAME", name="convt4")
            convt4 = tf.contrib.layers.batch_norm(convt4, is_training=is_training)
            convt4 = leaky_relu(convt4)

            convt5 = convt2d(convt4, (None, self.image_size, self.image_size, 3), kernal=(5, 5)
                             , strides=(2, 2), padding="SAME", name="convt5")
            convt5 = tf.nn.tanh(convt5)

            return convt5

    def langevin_dynamics(self, sess, samples, gradient, batch_id):
        for i in range(self.sample_steps):
            noise = np.random.randn(*samples.shape)
            grad = sess.run(gradient, feed_dict={self.syn: samples})
            samples = samples - 0.5 * self.delta * self.delta * (samples / self.refsig / self.refsig - grad) \
                      + self.delta * noise
            self.pbar.update(batch_id * self.sample_steps + i + 1)
        return samples

    def train(self, sess):

        gen_res = self.generator(self.z, reuse=False)

        obs_res = self.descriptor(self.obs, reuse=False)
        syn_res = self.descriptor(self.syn, reuse=True)
        sample_loss = tf.reduce_sum(syn_res)

        recon_err_mean, recon_err_update = tf.contrib.metrics.streaming_mean_squared_error(
            tf.reduce_mean(self.syn, axis=0), tf.reduce_mean(self.obs, axis=0))

        dLdI = tf.gradients(sample_loss, self.syn)[0]

        # Prepare training data
        train_data = DataSet(self.data_path, image_size=self.image_size)

        num_batches = int(math.ceil(len(train_data) / self.batch_size))

        # descriptor variables
        des_vars = [var for var in tf.trainable_variables() if var.name.startswith('des')]
        #
        des_loss = tf.subtract(tf.reduce_mean(syn_res, axis=0), tf.reduce_mean(obs_res, axis=0))
        des_loss_mean, des_loss_update = tf.contrib.metrics.streaming_mean(des_loss)

        des_optim = tf.train.AdamOptimizer(self.d_lr, beta1=self.beta1)
        des_grads_vars = des_optim.compute_gradients(des_loss, var_list=des_vars)
        des_grads = [tf.reduce_mean(tf.abs(grad)) for (grad, var) in des_grads_vars if '/w' in var.name]
        # update by mean of gradients
        apply_d_grads = des_optim.apply_gradients(des_grads_vars)

        # generator variables
        gen_vars = [var for var in tf.trainable_variables() if var.name.startswith('gen')]

        gen_loss = tf.reduce_mean(1.0 / (2 * self.sigma * self.sigma) * tf.square(self.obs - gen_res), axis=0)
        gen_loss_mean, gen_loss_update = tf.contrib.metrics.streaming_mean(tf.reduce_mean(gen_loss))

        gen_optim = tf.train.AdamOptimizer(self.g_lr, beta1=self.beta1)
        gen_grads_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_vars)
        gen_grads = [tf.reduce_mean(tf.abs(grad)) for (grad, var) in gen_grads_vars if '/w' in var.name]
        apply_g_grads = gen_optim.apply_gradients(gen_grads_vars)

        tf.summary.scalar('des_loss', des_loss_mean)
        tf.summary.scalar('gen_loss', gen_loss_mean)
        tf.summary.scalar('recon_err', recon_err_mean)

        summary_op = tf.summary.merge_all()

        # initialize training
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        sample_results = np.random.randn(self.num_chain * num_batches, self.image_size, self.image_size, 3)

        saver = tf.train.Saver(max_to_keep=50)

        writer = tf.summary.FileWriter(self.log_dir, sess.graph)

        for epoch in range(self.num_epochs):

            widgets = ["Epoch #%d|" % epoch, Percentage(), Bar(), ETA()]
            self.pbar = ProgressBar(maxval=num_batches * self.sample_steps, widgets=widgets)
            self.pbar.start()

            for i in range(num_batches):
                obs_data = train_data[i * self.batch_size:min(len(train_data), (i + 1) * self.batch_size)]
                z_vec = np.random.randn(self.num_chain, self.z_size)

                syn = sess.run(gen_res, feed_dict={self.z: z_vec})
                # generate synthesized images
                syn = self.langevin_dynamics(sess, syn, dLdI, i)
                # learn D net
                sess.run([des_loss_update, apply_d_grads], feed_dict={self.obs: obs_data, self.syn: syn})

                # learn G net
                sess.run([gen_loss_update, apply_g_grads], feed_dict={self.obs: syn, self.z: z_vec})

                # Compute MSE
                sess.run(recon_err_update, feed_dict={self.obs: obs_data, self.syn: syn})

                sample_results[i * self.num_chain:(i + 1) * self.num_chain] = syn

            self.pbar.finish()

            [des_loss_avg, gen_loss_avg, mse, summary] = sess.run([des_loss_mean, gen_loss_mean,
                                                                   recon_err_mean,
                                                                   summary_op])

            print('Epoch #{:d}, descriptor loss: {:.4f},  generator loss: {:.4f}, Avg MSE: {:4.4f}'.format(epoch,
                                                                                                           des_loss_avg,
                                                                                                           gen_loss_avg,
                                                                                                           mse))
            writer.add_summary(summary, epoch)

            if epoch % self.log_step == 0:
                if not os.path.exists(self.sample_dir):
                    os.makedirs(self.sample_dir)
                saveSampleResults(sample_results, "%s/des%03d.png" % (self.sample_dir, epoch), col_num=12)

                z_test = np.random.randn(self.num_chain, self.z_size)
                g_test = sess.run(gen_res, feed_dict={self.z: z_test})
                saveSampleResults(g_test, "%s/gen%03d.png" % (self.sample_dir, epoch), col_num=12)

                if not os.path.exists(self.model_dir):
                    os.makedirs(self.model_dir)
                saver.save(sess, "%s/%s" % (self.model_dir, 'model.ckpt'), global_step=epoch)
